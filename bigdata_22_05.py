# -*- coding: utf-8 -*-
"""BigData_22_05.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UaXcJJOT9rQ5xDolxhobcZJOau0TlNK8

# Instalando o Kaggle
"""

!pip install kaggle

"""# Realizando o upload do token de segurança de comunicação com o kaggle"""

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))
  
# Then move kaggle.json into the folder where the API expects to find it.
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json

!kaggle competitions list

"""# Fazendo o download do dataset do kaggle"""

!kaggle datasets download -d saraivaufc/automatic-weather-stations-brazil

"""# Descompactando o dataset baixado"""

!unzip automatic-weather-stations-brazil.zip

"""# Configurando o Spark"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://apache.osuosl.org/spark/spark-3.0.2/spark-3.0.2-bin-hadoop3.2.tgz
!tar xf spark-3.0.2-bin-hadoop3.2.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.0.2-bin-hadoop3.2"

import findspark
findspark.init()

from pyspark.sql import SparkSession

spark = SparkSession.builder.master("local[*]").getOrCreate()

spark

# Criando uma sessão no Spark
spark = SparkSession.builder.getOrCreate()

from datetime import datetime, date  
from pyspark.sql import Row

"""Carregando dataset no spark"""

linhas = spark.sparkContext.textFile('automatic_weather_stations_inmet_brazil_2000_2021.csv')

df = linhas.map(lambda line:line.split(';'))

print(df)

#Colunas desejadas
def func(lista):
  return lista[0], lista[1], lista[2], lista[3], lista[4], lista[8], lista[16], lista[19]
 
df_limpo = df.map(func)

df_limpo.take(2)

#tentar otimizar filtro com isso aqui
filter = ['A306','A314','A315','A319','A324','A325','A332','A339','A342','A347','A358','A359','A360','A368','A369']

df_ceara = df_limpo.filter(lambda x: 'A306' in x or 'A314' in x or 'A315' in x or 'A319' in x or 'A324' in x or 'A325' in x or 'A332' in x or 'A339' in x or 'A342' in x or 'A347' in x or 'A358' in x or 'A359' in x or 'A360' in x or 'A368' in x or 'A369' in x).collect()

#df_ceara = df_ceara.map(lambda line:line.split('['))

len(df_ceara)

from pyspark.sql import SparkSession
from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType, DateType, FloatType, TimestampType

schema = StructType([
    StructField('ESTACAO', StringType(), True),
    StructField('DATA', DateType(), True),
    StructField('HORA', TimestampType(), True),
    StructField('PRECIPITACAO TOTAL HORARIO (mm)', IntegerType(), True),
    StructField('PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB)', FloatType(), True),
    StructField('TEMPERATURA DO AR - BULBO SECO, HORARIA (C)', FloatType(), True),
    StructField('UMIDADE RELATIVA DO AR, HORARIA (%)', FloatType(), True),
    StructField('VENTO, VELOCIDADE HORARIA (m/s)', FloatType(), True),
])

# Convert list to RDD
rdd = spark.sparkContext.parallelize(df_ceara)
df = spark.createDataFrame(rdd,['ESTACAO',
  'DATA',
  'HORA',
  'PRECIPITACAO',
  'PRESSAO_ATMOSFERICA',
  'TEMPERATURA',
  'UMIDADE',
  'VENTO'])

# Create data frame
#df = spark.createDataFrame(rdd,schema)
#print(df.schema)
df.show()

df_plotar = df

df_plotar.show()

df_plotar = df_plotar.toPandas()

type(df)

df.createOrReplaceTempView('tableA')
print(spark.sql('select * from tableA').show())

"""#Tratamento de Dados (ETL)

#### Tratamento de valores misssing/Verificando valores Missing
"""

from pyspark.sql.functions import isnan, when, count, col
df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()

#Convertendo o data Frame typos
from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType, DoubleType, LongType
dfC_plots = df.withColumn("ESTACAO",df.ESTACAO.cast(StringType())).withColumn("DATA",df.DATA.cast(LongType())).withColumn("HORA",df.HORA.cast(DoubleType())).withColumn("PRECIPITACAO",df.PRECIPITACAO.cast(DoubleType())).withColumn("PRESSAO_ATMOSFERICA",df.PRESSAO_ATMOSFERICA.cast(DoubleType())).withColumn("TEMPERATURA",df.TEMPERATURA.cast(DoubleType())).withColumn("UMIDADE",df.UMIDADE.cast(DoubleType())).withColumn("VENTO",df.VENTO.cast(DoubleType()))
dfC_plots = dfC_plots.drop('ESTACAO', 'DATA', 'HORA')
dfC_plots = dfC_plots.toPandas()

#Removação de valores nulls para plot
#Tratamento de dados para plot
dfC_plots.head()

#Drop de valores nulls
dfC_plots.dropna(inplace=True)
#Verificação de valores Nulus
dfC_plots.isnull().values.any()

"""Drop e verificação de valores missing se julgado necessário

df.na.drop()

df.describe().show()
"""

df

#Convertendo o data Frame
from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType, DateType, FloatType, TimestampType, LongType
dfNovo = df.withColumn("ESTACAO",df.ESTACAO.cast(StringType())).withColumn("DATA",df.DATA.cast(DateType())).withColumn("HORA",df.HORA.cast(IntegerType())).withColumn("PRECIPITACAO",df.PRECIPITACAO.cast(FloatType())).withColumn("PRESSAO_ATMOSFERICA",df.PRESSAO_ATMOSFERICA.cast(FloatType())).withColumn("TEMPERATURA",df.TEMPERATURA.cast(FloatType())).withColumn("UMIDADE",df.UMIDADE.cast(FloatType()))

dfNovo = dfNovo.withColumn("VENTO", df.VENTO.cast(FloatType()))

dfNovo

dfNovo.show()

"""# Analise Descritiva dos Dados (Data analysis)

#####Importando Bibliotecas: Data analysis
"""

import numpy as np
import random as rnd
from pyspark.sql.window import Window
import pyspark.sql.functions as F

dfNovo.describe().show()

dfNovo.select('ESTACAO').describe().show()

dfNovo.select('DATA').describe().show()

dfNovo.select('HORA').describe().show()

dfNovo.select('PRECIPITACAO').describe().show()

dfNovo.select('PRESSAO_ATMOSFERICA').describe().show()

dfNovo.select('TEMPERATURA').describe().show()

dfNovo.select('UMIDADE').describe().show()

dfNovo.select('VENTO').describe().show()

float(dfNovo.describe("VENTO").filter("summary = 'max'").select("VENTO").collect()[0].asDict()['VENTO'])

"""######Percentil dos dados

######Percentis são medidas que dividem a amostra (por ordem crescente dos dados) em 100 partes, cada uma com uma percentagem de dados aproximadamente igual.O k-ésimo percentil Pk é o valor x (xk) que corresponde à frequência cumulativa de N .k/100, onde N é o tamanho amostral. Analise do percentil dos dados para ver o quão proximos estão as camadas de amostras

"""

#Analise pela hora Hora
df_teste = df.select("ESTACAO","DATA","HORA", F.percent_rank().over(Window.partitionBy().orderBy(df['ESTACAO'])).alias("percent_rank"))
#df_teste.show()
#df_teste.show(10000)

#Analise pela hora Hora
df_teste = df.select("ESTACAO","DATA","HORA", F.percent_rank().over(Window.partitionBy().orderBy(df["DATA"])).alias("percent_rank"))
#df_teste.show(10000)

#Analise pela hora Hora
df_teste = df.select("ESTACAO","DATA","HORA", F.percent_rank().over(Window.partitionBy().orderBy(df["HORA"])).alias("percent_rank"))
#df_teste.show(10000)

#Analise pela hora "TEMPERATURA DO AR - BULBO SECO, HORARIA (C)"
df_teste = df.select("TEMPERATURA","UMIDADE","VENTO", F.percent_rank().over(Window.partitionBy().orderBy(df["TEMPERATURA"])).alias("percent_rank"))
#df_teste.show(10000)

#Analise pela hora "UMIDADE RELATIVA DO AR, HORARIA (%)"
df_teste = df.select("TEMPERATURA","UMIDADE","VENTO", F.percent_rank().over(Window.partitionBy().orderBy(df["UMIDADE"])).alias("percent_rank"))
#df_teste.show(10000)

#Analise pela hora Umidade Relativa
df_teste = df.select("TEMPERATURA","UMIDADE","VENTO", F.percent_rank().over(Window.partitionBy().orderBy(df["VENTO"])).alias("percent_rank"))
#df_teste.show(10000)

#Percentis são medidas que dividem a amostra (por ordem crescente dos dados) em 100 partes, cada uma com uma percentagem de dados aproximadamente igual.
#O k-ésimo percentil Pk é o valor x (xk) que corresponde à frequência cumulativa de N .k/100, onde N é o tamanho amostral. Analise do percentil dos dados 
#para ver o quão proximos estão as camadas de amostras
df_teste = df.select("ESTACAO","DATA","HORA", "PRECIPITACAO", "TEMPERATURA", "PRESSAO_ATMOSFERICA", "TEMPERATURA", "UMIDADE","VENTO", F.percent_rank().over(Window.partitionBy().orderBy(df["DATA"])).alias("percent_rank"))
df_teste.show(100)

"""# Data Analytics (Vizualição dos Dados)

Conversanto os tipos de dados
"""

# Commented out IPython magic to ensure Python compatibility.
# Importando bibliotecas
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

"""Plotando gratico dos tipos de dados"""

dfNovo.show()

df_plots = dfNovo.toPandas()
dfC_plots.plot(figsize=(10,5))
display(plt.show())
#Certos dados não são registrados em determinados periodos
#Plot da distribuição dos dados

df_plotar.PRESSAO_ATMOSFERICA.hist(bins = 20)
plt.xlabel("Valores de Pressão Atmosferica")
plt.ylabel("Quantidade de Ocorrencias")
plt.title("Distribuição Relativa da Atmosferica")
plt.show()

df_plotar.PRECIPITACAO.hist(bins = 20)
plt.xlabel("Valores de Precipitação")
plt.ylabel("Quantidade de Ocorrencias")
plt.title("Distribuição Relativa da Precipitação")
plt.show()
#Concluimos que existe um grande periodo de precipitação no estado do ceara acompanhando de pequenas preciptitações após grande periodo de precipitação

df_plotar.TEMPERATURA.hist(bins = 20)
plt.xlabel("Valores de Temperatura")
plt.ylabel("Quantidade de Ocorrencias")
plt.title("Distribuição Relativa da Temperatura")
plt.show()

df_plotar.VENTO.hist(bins = 25)
plt.xlabel("Valores de Vento")
plt.ylabel("Quantidade de Ocorrencias")
plt.title("Distribuição Relativa da Vento")
plt.show()

df_plotar.UMIDADE.hist(bins = 20)
plt.xlabel("Valores de Umidade")
plt.ylabel("Quantidade de Ocorrencias")
plt.title("Distribuição Relativa da Umidade")
plt.show()

# Crie um Boxplot - Outliers PRECIPITACAO
# BoxPlot 
sns.set_style("whitegrid")
fig, ax = plt.subplots(figsize=(15,10))
sns.boxplot(x="PRECIPITACAO", y="HORA", data=df_plots)
ax.text(300.0,2000,"Análise de Outliers",fontsize=18,color="r",ha="center", va="center")
ax.xaxis.set_label_text("PRECIPITACAO",fontdict= {'size':28})
ax.yaxis.set_label_text("HORA",fontdict= {'size':28})
plt.show()

# Crie um Boxplot - Outliers PRECIPITACAO
# BoxPlot 
sns.set_style("whitegrid")
fig, ax = plt.subplots(figsize=(15,10))
sns.boxplot(x="TEMPERATURA", y="HORA", data=df_plots)
ax.text(300.0,2000,"Análise de Outliers",fontsize=18,color="r",ha="center", va="center")
ax.xaxis.set_label_text("TEMPERATURA",fontdict= {'size':28})
ax.yaxis.set_label_text("HORA",fontdict= {'size':28})
plt.show()

# Crie um Boxplot - Outliers VENTO
# BoxPlot 
sns.set_style("whitegrid")
fig, ax = plt.subplots(figsize=(20,15))
sns.boxplot(x="UMIDADE", y="HORA", data=df_plots)
ax.text(120.0,2000,"Análise de Outliers",fontsize=18,color="r",ha="center", va="center")
ax.xaxis.set_label_text("HORA",fontdict= {'size':28})
ax.yaxis.set_label_text("UMIDADE",fontdict= {'size':28})
plt.show()

# Crie um Boxplot - Outliers VENTO
# BoxPlot 
sns.set_style("whitegrid")
fig, ax = plt.subplots(figsize=(20,15))
sns.boxplot(x="VENTO", y="HORA", data=df_plots)
ax.text(140.0,2000,"Análise de Outliers",fontsize=18,color="r",ha="center", va="center")
ax.xaxis.set_label_text("HORA",fontdict= {'size':28})
ax.yaxis.set_label_text("VENTO",fontdict= {'size':28})
plt.show()

#Convertendo o data Frame
from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType, DateType, FloatType, TimestampType, LongType
df_plotar = df.withColumn("ESTACAO",df.ESTACAO.cast(StringType())).withColumn("DATA",df.DATA.cast(DateType())).withColumn("HORA",df.HORA.cast(IntegerType())).withColumn("PRECIPITACAO",df.PRECIPITACAO.cast(FloatType())).withColumn("PRESSAO_ATMOSFERICA",df.PRESSAO_ATMOSFERICA.cast(FloatType())).withColumn("TEMPERATURA",df.TEMPERATURA.cast(FloatType())).withColumn("UMIDADE",df.UMIDADE.cast(FloatType()))

df_plotar = df_plotar.toPandas()

fig, ax = plt.subplots(figsize=(10,8))
sns.distplot(df_plotar["PRECIPITACAO"], color="#33cc33",kde=True, ax=ax)
ax.set_title('Distribuição de PRECIPITACAO com base na Densidade', fontsize= 20)
plt.ylabel("Densidade (PRECIPITACAO)", fontsize= 20)
plt.xlabel("PRECIPITACAO", fontsize= 20)
plt.show()

fig, ax = plt.subplots(figsize=(10,8))
sns.distplot(df_plotar["PRESSAO_ATMOSFERICA"], color="#33cc33",kde=True, ax=ax)
ax.set_title('Distribuição de PRESSAO_ATMOSFERICA com base na Densidade', fontsize= 20)
plt.ylabel("Densidade (PRESSAO_ATMOSFERICA)", fontsize= 20)
plt.xlabel("PRESSAO_ATMOSFERICA", fontsize= 20)
plt.show()

fig, ax = plt.subplots(figsize=(10,8))
sns.distplot(df_plots["TEMPERATURA"], color="#33cc33",kde=True, ax=ax)
ax.set_title('Distribuição de TEMPERATURA com base na Densidade', fontsize= 20)
plt.ylabel("Densidade (TEMPERATURA)", fontsize= 20)
plt.xlabel("TEMPERATURA", fontsize= 20)
plt.show()

fig, ax = plt.subplots(figsize=(10,8))
sns.distplot(df_plots["UMIDADE"], color="#33cc33",kde=True, ax=ax)
ax.set_title('Distribuição de UMIDADE com base na Densidade', fontsize= 20)
plt.ylabel("Densidade (UMIDADE)", fontsize= 20)
plt.xlabel("UMIDADE", fontsize= 20)
plt.show()

fig, ax = plt.subplots(figsize=(10,8))
sns.distplot(df_plots["VENTO"], color="#33cc33",kde=True, ax=ax)
ax.set_title('Distribuição de VENTO com base na Densidade', fontsize= 20)
plt.ylabel("Densidade (VENTO)", fontsize= 20)
plt.xlabel("VENTO", fontsize= 20)
plt.show()

def plot_corr(df, size=12):
    corr = df.corr()    
    fig, ax = plt.subplots(figsize = (size, size))
    ax.matshow(corr)  
    plt.xticks(range(len(corr.columns)), corr.columns) 
    plt.yticks(range(len(corr.columns)), corr.columns)

#print do grafico gerado
plot_corr(dfC_plots)

"""# ETL (Data Analytics): Processamento Pos-Aprendizado de Dados

#### Etapa de processamento de dados apos descoberta e analise dos dados, assim como limpeza, tratamento, analise de outliers, sujeira e valores negativos (Dados erroneos)
"""

#Convertendo o data Frame
from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType, DateType, FloatType, TimestampType, LongType, DoubleType
dfCorrelacao = df.withColumn("ESTACAO",df.ESTACAO.cast(StringType())).withColumn("DATA",df.DATA.cast(LongType())).withColumn("HORA",df.HORA.cast(IntegerType())).withColumn("PRECIPITACAO",df.PRECIPITACAO.cast(FloatType())).withColumn("PRESSAO_ATMOSFERICA",df.PRESSAO_ATMOSFERICA.cast(FloatType())).withColumn("TEMPERATURA",df.TEMPERATURA.cast(FloatType())).withColumn("UMIDADE",df.UMIDADE.cast(FloatType())).withColumn("VENTO",df.VENTO.cast(FloatType()))
dfCorrelacao = dfCorrelacao.drop('ESTACAO', 'DATA')

dfCorrelacao

dfCorrelacao.show()

#Convertendo o data Frame typos
from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType, DoubleType
dfCorrelacao2 = df.withColumn("ESTACAO",df.ESTACAO.cast(StringType())).withColumn("DATA",df.DATA.cast(LongType())).withColumn("HORA",df.HORA.cast(DoubleType())).withColumn("PRECIPITACAO",df.PRECIPITACAO.cast(DoubleType())).withColumn("PRESSAO_ATMOSFERICA",df.PRESSAO_ATMOSFERICA.cast(DoubleType())).withColumn("TEMPERATURA",df.TEMPERATURA.cast(DoubleType())).withColumn("UMIDADE",df.UMIDADE.cast(DoubleType())).withColumn("VENTO",df.VENTO.cast(DoubleType()))
dfCorrelacao2 = dfCorrelacao2.drop('ESTACAO', 'DATA', 'HORA')
dfCorrelacao2 = dfCorrelacao2.toPandas()

dfCorrelacao2

"""##Vetor de Densidade

##### Um DenseVector é a implementação "óbvia" de um Vector, com uma variações.Os dados subjacentes podem ter mais dados do que o Vetor, representados o deslocamento na matriz (apatir do 0º elemento), e cada passo mostra a distância dos elementos um do outro.
"""

from pyspark.ml.feature import VectorAssembler
assembler = VectorAssembler(
  inputCols = ['HORA', 'PRECIPITACAO', 'PRESSAO_ATMOSFERICA', 'TEMPERATURA', 'UMIDADE', 'VENTO']
  , outputCol = "Dense Vector"
)

output = assembler.transform(dfCorrelacao)

output.show()

#PROCESSAMENTO E TRATAMENTO DE DADOS FINAIS PARA MACHINE LEARNING (TRATAMENTO REALIZADO APOS DESCOBERTA DE DADOS MAIS PROFUNDA)
from pyspark.ml.feature import VectorAssembler
from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType, DateType, FloatType, TimestampType, LongType, DoubleType

dfNovo = df.withColumn("ESTACAO",df.ESTACAO.cast(StringType())).withColumn("DATA",df.DATA.cast(DateType())).withColumn("HORA",df.HORA.cast(IntegerType())).withColumn("PRECIPITACAO",df.PRECIPITACAO.cast(DoubleType())).withColumn("PRESSAO_ATMOSFERICA",df.PRESSAO_ATMOSFERICA.cast(DoubleType())).withColumn("TEMPERATURA",df.TEMPERATURA.cast(DoubleType())).withColumn("UMIDADE",df.UMIDADE.cast(DoubleType()))
dfNovo = dfNovo.withColumn("VENTO", df.VENTO.cast(FloatType()))
dfNovo = dfNovo.na.drop()

#TRATAMENTO DOS VALORES -99999
lista = dfNovo.filter((dfNovo.PRECIPITACAO  > 0) &  (dfNovo.PRESSAO_ATMOSFERICA  > 0)  &  (dfNovo.TEMPERATURA  > 0)  &  (dfNovo.UMIDADE  > 0)  &  (dfNovo.VENTO  > 0)).collect()
rdd_filtrado = spark.sparkContext.parallelize(lista)
 

assembler = VectorAssembler(
  inputCols = ['PRECIPITACAO', 'PRESSAO_ATMOSFERICA', 'TEMPERATURA', 'UMIDADE', 'VENTO']
  , outputCol = "features"
)
 
df_filtrado = spark.createDataFrame(rdd_filtrado,['ESTACAO',
  'DATA',
  'HORA',
  'PRECIPITACAO',
  'PRESSAO_ATMOSFERICA',
  'TEMPERATURA',
  'UMIDADE',
  'VENTO'])

assembled = assembler.transform(df_filtrado)

"""##Correlação das variaveis

#####Os coeficientes de correlação são usados para medir as relações entre variáveis, representando o entendimento da forma que as variaveis se comportam em em relação as outras, permitindo assim na analise ser identificada as relações entre a variabilidade de ambas.
"""

dfCorrelacao2.corr()

"""# Importando bibliotecas de Machine Learning

###### Importando bibliotecas de machine learning: Bibliotecas Sklearn
"""

import sklearn as sk
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn import metrics
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
import sklearn 
print (sklearn.__version__)
import numpy as np
from sklearn.impute import SimpleImputer

"""#### Apos a separação dos dados de treino e teste para o modelo. 
- Criação dos dados de treino e teste  do modelo

## Machine Learning Clusterização usando KMeans

######IMPORTANDO BIBLIOTECA
"""

#IMPORTANDO BIBLIOTECAS
from pyspark.sql.functions import lit  
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator

"""##### Incializando modelo, treinando e realizando predição"""

#CLUSTERING#
kmeans = KMeans(k=2, seed=666)  # 2 clusters
model = kmeans.fit(assembled.select('features'))
result = model.transform(assembled)
silhouette_score=[]
evaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='features', \
                                metricName='silhouette', distanceMeasure='squaredEuclidean')
score=evaluator.evaluate(result)
silhouette_score.append(score)

#PRINT DO SCORE DA SILHUETA
print("Silhouette Score:",score)

"""###Melhor Simulheta/ Quantidade de Clusters

##### Calcular o Indice 'n' clusteres
"""

import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import make_blobs
# Importando os arquivos
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
# Importando os arquivos
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

pdf2 = result.toPandas()
pdf2.drop(['ESTACAO'],axis=1,inplace=True)
pdf2.drop(['DATA'],axis=1,inplace=True)
pdf2.drop(['HORA'],axis=1,inplace=True)
pdf2.head()

# Definindo a quantidade de clusters
# Depois de varias analises de processamento de dados o numero de clusters escolhidos será 2
# definição do objeto
kmeans = KMeans(n_clusters=2)

#transferindo os dados para variavel a ser lida no modelo do kmeans
Kfit = pdf2.iloc[:, 0:4].values

kmeans.fit(Kfit)

preds = kmeans.predict(Kfit)
silhouette_score(Kfit,preds)

#Calculo para melhor coeficiente para uso dos clusteres
for k in range(2, 5):
    kmeans_ = KMeans(n_clusters=k, random_state=10)
    kmeans_.fit(Kfit)
    print(k, silhouette_score(Kfit, kmeans_.predict(Kfit)))

#================================

# Import required packages
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

categorical_features = ['VENTO']
continuous_features = ['PRECIPITACAO', 'PRESSAO_ATMOSFERICA', 'TEMPERATURA', 'UMIDADE']

for col in categorical_features:
    dummies = pd.get_dummies(pdf2[col], prefix=col)
    data = pd.concat([pdf2, dummies], axis=1)
    data.drop(col, axis=1, inplace=True)
data.head()

mms = MinMaxScaler()
mms.fit(Kfit)
data_transformed = mms.transform(Kfit)

Sum_of_squared_distances = []
K = range(1,15)
for k in K:
    km = KMeans(n_clusters=k)
    km = km.fit(data_transformed)
    Sum_of_squared_distances.append(km.inertia_)

plt.plot(K, Sum_of_squared_distances, 'bx-')
plt.xlabel('k')
plt.ylabel('---------------------')
plt.title('Grafico de Cotovelo')
plt.show()

"""#Vizualição dos dados """

import seaborn as sns
pdf = result.toPandas()

sns.scatterplot(data=pdf, x='TEMPERATURA', y='VENTO', hue='prediction')

sns.scatterplot(data=pdf, x='TEMPERATURA', y='UMIDADE', hue='prediction')

sns.scatterplot(data=pdf, x='TEMPERATURA', y='PRECIPITACAO', hue='prediction')

sns.scatterplot(data=pdf, x='PRESSAO_ATMOSFERICA', y='PRECIPITACAO', hue='prediction')

sns.scatterplot(data=pdf, x='PRESSAO_ATMOSFERICA', y='TEMPERATURA', hue='prediction')

sns.scatterplot(data=pdf, x='PRESSAO_ATMOSFERICA', y='VENTO', hue='prediction')

sns.scatterplot(data=pdf, x='UMIDADE', y='VENTO', hue='prediction')

"""### Plots DF Limpo (Histograma)

##### Plot dos graficos apos tratamento
"""

df_filtrado = df_filtrado.toPandas()

df_filtrado.plot(figsize=(10,5))
display(plt.show())



df_filtrado.PRECIPITACAO.hist(bins = 20)
plt.xlabel("Valores de Precipitação")
plt.ylabel("Quantidade de Ocorrencias")
plt.title("Distribuição Relativa da Precipitação")
plt.show()

df_filtrado.TEMPERATURA.hist(bins = 20)
plt.xlabel("Valores de Temperatura")
plt.ylabel("Quantidade de Ocorrencias")
plt.title("Distribuição Relativa da Temperatura")
plt.show()

df_filtrado.VENTO.hist(bins = 25)
plt.xlabel("Valores de Vento")
plt.ylabel("Quantidade de Ocorrencias")
plt.title("Distribuição Relativa da Vento")
plt.show()

df_filtrado.UMIDADE.hist(bins = 20)
plt.xlabel("Valores de Umidade")
plt.ylabel("Quantidade de Ocorrencias")
plt.title("Distribuição Relativa da Umidade")
plt.show()

fig, ax = plt.subplots(figsize=(10,8))
sns.distplot(df_filtrado["PRESSAO_ATMOSFERICA"], color="#33cc33",kde=True, ax=ax)
ax.set_title('Distribuição de PRESSAO_ATMOSFERICA com base na Densidade', fontsize= 20)
plt.ylabel("Densidade (PRESSAO_ATMOSFERICA)", fontsize= 20)
plt.xlabel("PRESSAO_ATMOSFERICA", fontsize= 20)
plt.show()

fig, ax = plt.subplots(figsize=(10,8))
sns.distplot(df_filtrado["TEMPERATURA"], color="#33cc33",kde=True, ax=ax)
ax.set_title('Distribuição de TEMPERATURA com base na Densidade', fontsize= 20)
plt.ylabel("Densidade (TEMPERATURA)", fontsize= 20)
plt.xlabel("TEMPERATURA", fontsize= 20)
plt.show()

fig, ax = plt.subplots(figsize=(10,8))
sns.distplot(df_filtrado["UMIDADE"], color="#33cc33",kde=True, ax=ax)
ax.set_title('Distribuição de UMIDADE com base na Densidade', fontsize= 20)
plt.ylabel("Densidade (UMIDADE)", fontsize= 20)
plt.xlabel("UMIDADE", fontsize= 20)
plt.show()

fig, ax = plt.subplots(figsize=(10,8))
sns.distplot(df_filtrado["VENTO"], color="#33cc33",kde=True, ax=ax)
ax.set_title('Distribuição de VENTO com base na Densidade', fontsize= 20)
plt.ylabel("Densidade (VENTO)", fontsize= 20)
plt.xlabel("VENTO", fontsize= 20)
plt.show()